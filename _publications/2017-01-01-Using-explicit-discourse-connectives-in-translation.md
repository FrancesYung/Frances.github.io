---
title: "Using explicit discourse connectives in translation for implicit discourse relation classification"
collection: publications
category: conferences
permalink: /publication/2017-01-01-Using-explicit-discourse-connectives-in-translation
excerpt: 'Implicit discourse relation recognition is an extremely challenging task due to the lack of indicative connectives. Various neural network architectures have been proposed for this task recently, but most of them suffer from the shortage of labeled data. In this paper, we address this problem by procuring additional training data from parallel corpora: When humans translate a text, they sometimes add connectives (a process known as explicitation). We automatically back-translate it into an English connective and use it to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current state-of-the-art.'
date: 2017-01-01
venue: 'IJPNLP'
paperurl: 'https://aclanthology.org/I17-1049/'
citation: 'Wei Shi, Frances Yung, Raphael Rubino, Vera DembergIJPNLP 2017'
authors: 'Wei Shi, Frances Yung, Raphael Rubino, Vera Demberg'
---
Wei Shi, Frances Yung, Raphael Rubino, Vera Demberg

<a href='https://aclanthology.org/I17-1049/'>Download paper here</a>

Implicit discourse relation recognition is an extremely challenging task due to the lack of indicative connectives. Various neural network architectures have been proposed for this task recently, but most of them suffer from the shortage of labeled data. In this paper, we address this problem by procuring additional training data from parallel corpora: When humans translate a text, they sometimes add connectives (a process known as explicitation). We automatically back-translate it into an English connective and use it to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current state-of-the-art.
